{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset in D:\\DATA\\APPLE\\Roboflow\\\n",
    "\n",
    "# folder structure\n",
    "# /train\n",
    "#   _annotations.coco.json\n",
    "#   img1.jpg\n",
    "#   img2.jpg\n",
    "#   ...\n",
    "# /valid\n",
    "#   _annotations.coco.json\n",
    "#   img1.jpg\n",
    "#   img2.jpg\n",
    "#   ...\n",
    "\n",
    "import os\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define paths\n",
    "train_images_dir = \"D:/DATA/APPLE/Roboflow/train\"\n",
    "train_annotations_file = os.path.join(train_images_dir, \"_annotations.coco.json\")\n",
    "valid_images_dir = \"D:/DATA/APPLE/Roboflow/valid\"\n",
    "valid_annotations_file = os.path.join(valid_images_dir, \"_annotations.coco.json\")\n",
    "\n",
    "# Define a basic transform (resizing, normalization, etc. as needed)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(\n",
    "        #     (224, 224)\n",
    "        # ),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Create custom dataset for COCO format\n",
    "class CustomCocoDataset(CocoDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        # Preprocess target as needed\n",
    "\n",
    "        bbox = [ann[\"bbox\"] for ann in target]  # Get all bboxes in the target\n",
    "        category_id = [ann[\"category_id\"] for ann in target]  # Get all category IDs\n",
    "\n",
    "        # Return image, bbox, and category information as a dictionary\n",
    "        count = len(bbox)\n",
    "        return img, count\n",
    "    # {\"bboxes\": bbox, \"category_ids\": category_id}\n",
    "\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    def _view(self,index):\n",
    "        \"\"\"to visualize the annotated image\n",
    "\n",
    "        Args:\n",
    "            index (int): for image at specific index\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 4,  2,  5, 18,  4,  1,  4,  8,  3, 12])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([2, 4, 2, 1, 8, 7, 5, 2, 1, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 1, 1, 1, 5, 1, 1, 1, 1, 5])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  2,  2,  1,  1, 10,  3,  1,  1,  3])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([2, 1, 6, 4, 1, 1, 4, 3, 1, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([6, 1, 1, 2, 2, 1, 6, 3, 1, 5])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1,  9, 15,  3,  1,  1,  2,  1,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1,  2,  3,  6,  1,  1,  3, 17,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3, 11,  5,  5,  2,  5,  3, 10,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1, 12,  5, 15,  6,  1,  1,  1, 14,  2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 1, 5, 1, 2, 1, 1, 2, 1, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([4, 3, 3, 1, 3, 5, 7, 2, 1, 6])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 4,  1,  1, 15,  1,  1,  1,  1,  1,  2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([4, 1, 2, 3, 9, 1, 3, 2, 1, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([5, 3, 1, 1, 5, 8, 1, 2, 1, 5])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1,  1,  1,  3,  1,  1, 16, 10,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  1,  7, 23, 13,  4,  9,  1,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([7, 2, 5, 1, 7, 1, 5, 1, 3, 3])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 4,  1,  1,  1,  4,  2,  4,  2, 13,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([7, 2, 3, 5, 4, 2, 3, 3, 4, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 2,  1,  9,  2,  4, 12,  3, 12,  5, 22])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1,  5,  1,  2,  6, 10,  1,  1,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 2,  1,  2,  1, 12,  3,  1,  1,  1,  3])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1, 12,  6,  8,  1,  7,  6, 14,  1,  4])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  1, 12,  1,  1,  1,  4,  4,  6,  2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 2,  2,  2, 17, 18,  2,  1,  5, 11,  4])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([12,  1,  1, 18,  8, 12,  4,  1,  4,  6])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 1, 3, 1, 4, 1, 2, 5, 2, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  1,  6,  4,  6,  1, 13,  1,  5,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  1,  8,  2,  1,  2,  9,  1, 11,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1,  3, 10, 11,  3,  1,  4,  1,  3,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 7, 1, 7, 3, 1, 2, 1, 2, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 1, 1, 4, 1, 3, 2, 1, 2, 4])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 5,  2,  1,  3,  1,  4, 11, 11,  3,  3])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([6, 1, 3, 4, 3, 1, 3, 3, 2, 2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([10,  1,  9,  1,  1,  6,  1,  1,  2,  6])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([9, 2, 1, 1, 1, 1, 2, 8, 3, 3])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  3,  8,  1,  3,  4,  1,  1, 10,  9])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 1, 2, 2, 3, 3, 1, 1, 1, 2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 2,  3,  1,  2, 20, 11,  1,  8,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  1,  9,  4,  4, 13,  8,  1,  8,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  7,  8,  7, 11,  1,  1,  5,  1,  2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 9,  2,  7,  6,  6,  1,  3, 10,  1,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([8, 3, 6, 7, 6, 3, 2, 1, 1, 4])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 1,  6,  2,  4,  8,  5, 10,  2,  2,  1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([4, 1, 2, 2, 2, 1, 2, 6, 1, 8])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 4,  1,  7,  3,  4,  3,  1, 10,  8,  7])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 4,  3,  1,  1,  7, 18,  2,  1,  1,  3])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([10,  2,  1,  2,  6,  7,  1, 17,  7,  4])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 2,  5, 12,  7, 10,  6,  4,  2,  5,  4])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 2,  8,  1,  1,  1,  4, 26,  9,  3,  2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([3, 2, 1, 3, 7, 5, 7, 1, 2, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([1, 6, 1, 3, 1, 1, 5, 5, 7, 1])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([3, 8, 6, 1, 1, 2, 2, 4, 3, 2])\n",
      "\n",
      "Images batch shape: torch.Size([10, 3, 192, 192])\n",
      "Targets batch: tensor([ 3,  1,  3,  4,  2, 10,  1,  1,  2,  4])\n",
      "\n",
      "Images batch shape: torch.Size([3, 3, 192, 192])\n",
      "Targets batch: tensor([2, 6, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize train and validation datasets\n",
    "train_dataset = CustomCocoDataset(\n",
    "    root=train_images_dir, \n",
    "    annFile=train_annotations_file, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "valid_dataset = CustomCocoDataset(\n",
    "    root=valid_images_dir, \n",
    "    annFile=valid_annotations_file, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    # num_workers=2,  # Adjust num_workers based on your system\n",
    "    # collate_fn=lambda x: tuple(\n",
    "    #     zip(*x)\n",
    "    # ),  # Use custom collate_fn to handle variable data sizes\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=40,\n",
    "    shuffle=False,\n",
    "    # num_workers=2,\n",
    "    # collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "# Sample usage to test the DataLoader\n",
    "for images, targets in train_loader:\n",
    "    print(\"Images batch shape:\", images.shape)  # Shape of each image in the batch\n",
    "    print(\"Targets batch:\", targets)  # Bounding boxes and category IDs\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 192, 192])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset[2][1])\n",
    "train_dataset[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vaibhav\\anaconda3\\envs\\fast\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vaibhav\\anaconda3\\envs\\fast\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "model = torchvision.models.resnet101(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "import torch.optim\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1581, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "\n",
    "        # print(images.shape, targets.shape)  # torch.Size([4, 3, 192, 192]) torch.Size([4])\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output = model(images)\n",
    "        # print(output.shape) # torch.Size([4, 1000])\n",
    "        loss = loss_fn(output,targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.36%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you're in the training loop\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode for accuracy calculation\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for images, targets in valid_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass to get model predictions\n",
    "        output = model(images)  # Output shape is [batch_size, 1000] for classification\n",
    "\n",
    "        # Get the predicted classes (argmax along the class dimension)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        # Update correct predictions count\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
